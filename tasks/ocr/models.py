from __future__ import annotations

# USU≈É to - spawn method ju≈º ustawiony w main.py!
# import multiprocessing
# multiprocessing.set_start_method('spawn',force=True)

import gc
import os
import signal
import time
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, Tuple
import pynvml

import torch
from transformers import AutoModelForVision2Seq, AutoProcessor

from .config import (
    DEFAULT_OCR_INSTRUCTION,
    DEVICE_STRATEGY as CFG_STRATEGY,
    GPU_MEM_LIMIT_GB,
    GPU_SELECT_MODE,
    OCR_MODEL_PATH,
    OCR_TIMEOUT_SECONDS,
    MAX_NEW_TOKENS,
    logger,
)

# Wy≈ÇƒÖczamy globalnie Flash‚ÄëAttention 2 ‚Äì znany source segfault√≥w na Ampere
os.environ["FLASH_ATTENTION_FORCE_DISABLED"] = "1"

print(f"üîç [OCR_MODELS] Importowano models.py w procesie PID={os.getpid()}")


class TimeoutError(Exception):
    """Sygnalizuje przekroczenie limitu czasu generacji jednej strony."""


# ---------------------------------------------------------------------------
#  Wyb√≥r najlepszej karty GPU (tryb single)
# ---------------------------------------------------------------------------


def get_gpu_performance(handle):
    try:
        pynvml.nvmlInit()
        cuda_cores = 10496  # RTX 3090 has 10496 CUDA cores; adjust if your model varies significantly
        clock_mhz = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_SM)
        logger.info(f"MHz {clock_mhz} ")
        return cuda_cores * clock_mhz
    except Exception as e:
        logger.error(f"B≈ÇƒÖd get_gpu_performance: {e}")
        return 0


def _pick_best_gpu(threshold_gb: int) -> int | None:
    try:
        pynvml.nvmlInit()
        best_gpu = None
        best_free = 0.0
        best_perf = 0

        for i in range(torch.cuda.device_count()):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)

            free, _ = torch.cuda.mem_get_info(i)
            free_gb = free / (1024 ** 3)

            if free_gb >= threshold_gb:
                performance = get_gpu_performance(handle)

                if (free_gb > best_free) or (free_gb == best_free and performance > best_perf):
                    best_gpu = i
                    best_free = free_gb
                    best_perf = performance

        pynvml.nvmlShutdown()
        return best_gpu
    except Exception as e:
        logger.error(f"B≈ÇƒÖd _pick_best_gpu: {e}")
        return 0  # Fallback na GPU 0


# ---------------------------------------------------------------------------
#  Model + processor ‚Äì singleton w pamiƒôci procesu
# ---------------------------------------------------------------------------

@lru_cache(maxsize=1)
def _load_once() -> Tuple[AutoModelForVision2Seq, AutoProcessor]:
    """≈Åaduje model OCR - raz na proces."""
    print(f"üîÑ [OCR_MODELS] ≈Åadowanie modelu w procesie PID={os.getpid()}")
    logger.info(f"üîÑ [OCR_MODELS] ≈Åadowanie modelu w procesie PID={os.getpid()}")

    try:
        # Sprawd≈∫ czy CUDA jest dostƒôpna
        if not torch.cuda.is_available():
            raise Exception("CUDA nie jest dostƒôpna!")

        print(f"üîç [OCR_MODELS] CUDA dostƒôpna, liczba GPU: {torch.cuda.device_count()}")
        logger.info(f"üîç [OCR_MODELS] CUDA dostƒôpna, liczba GPU: {torch.cuda.device_count()}")

        # Sprawd≈∫ czy to lokalny path czy Hugging Face Hub
        if "/" in OCR_MODEL_PATH and not Path(OCR_MODEL_PATH).exists():
            print(f"üåê [OCR_MODELS] Model z Hugging Face Hub: {OCR_MODEL_PATH}")
            logger.info(f"Model z Hugging Face Hub: {OCR_MODEL_PATH}")

            # Sprawd≈∫ cache HF
            try:
                from transformers import AutoConfig
                print(f"üîç [OCR_MODELS] Sprawdzam dostƒôpno≈õƒá modelu...")
                config = AutoConfig.from_pretrained(OCR_MODEL_PATH)
                print(f"‚úÖ [OCR_MODELS] Model dostƒôpny w HF Hub")
            except Exception as e:
                raise Exception(f"Model niedostƒôpny w HF Hub: {str(e)}")
        elif Path(OCR_MODEL_PATH).exists():
            print(f"üìÅ [OCR_MODELS] Model lokalny istnieje: {OCR_MODEL_PATH}")
        else:
            raise Exception(f"Model nie istnieje lokalnie ani w HF Hub: {OCR_MODEL_PATH}")

        strategy = CFG_STRATEGY

        if strategy == "single" and GPU_SELECT_MODE == "auto":
            gpu = _pick_best_gpu(GPU_MEM_LIMIT_GB)
            if gpu is None:
                logger.warning("Brak karty ‚â•%s GB ‚Äì prze≈ÇƒÖczam na device_map='auto'", GPU_MEM_LIMIT_GB)
                strategy = "auto"
                gpu = 0
        else:
            gpu = 0

        print(f"üîç [OCR_MODELS] Wybrano GPU: {gpu}, strategy: {strategy}")

        # Sprawd≈∫ pamiƒôƒá GPU przed ≈Çadowaniem
        try:
            free_mem, total_mem = torch.cuda.mem_get_info(gpu)
            free_gb = free_mem / (1024 ** 3)
            total_gb = total_mem / (1024 ** 3)
            print(f"üîç [OCR_MODELS] GPU {gpu} pamiƒôƒá: {free_gb:.2f}GB / {total_gb:.2f}GB dostƒôpne")

            if free_gb < 8.0:  # Model 7B potrzebuje ~14GB ale sprawdzamy minimum
                print(f"‚ö†Ô∏è [OCR_MODELS] UWAGA: Ma≈Ço pamiƒôci GPU ({free_gb:.2f}GB), model mo≈ºe nie za≈Çadowaƒá siƒô")
        except Exception as e:
            print(f"‚ö†Ô∏è [OCR_MODELS] Nie mo≈ºna sprawdziƒá pamiƒôci GPU: {e}")

        params: Dict[str, Any] = {"torch_dtype": torch.float16, "trust_remote_code": True}
        if strategy == "single":
            params["device_map"] = gpu  # ‚Üê kluczowa linia
            params["max_memory"] = {gpu: f"{GPU_MEM_LIMIT_GB}GiB"}
        else:
            params["device_map"] = "auto"

        print(f"üîç [OCR_MODELS] Parametry ≈Çadowania: {params}")
        logger.info("≈Åadowanie modelu z parametrami: %s", params)

        try:
            model = AutoModelForVision2Seq.from_pretrained(OCR_MODEL_PATH, **params).eval()
            print(f"‚úÖ [OCR_MODELS] Model za≈Çadowany pomy≈õlnie")
        except torch.cuda.OutOfMemoryError:
            print(f"‚ö†Ô∏è [OCR_MODELS] OOM - ponawiam z device_map='auto'")
            logger.warning("OOM ‚Äì ponawiam z device_map='auto'")
            params.pop("max_memory", None)
            params["device_map"] = "auto"
            model = AutoModelForVision2Seq.from_pretrained(OCR_MODEL_PATH, **params).eval()

        processor = AutoProcessor.from_pretrained(OCR_MODEL_PATH)
        print(f"‚úÖ [OCR_MODELS] Processor za≈Çadowany pomy≈õlnie")

        logger.info(f"‚úÖ [OCR_MODELS] Model i processor za≈Çadowane w procesie PID={os.getpid()}")
        return model, processor

    except Exception as e:
        error_msg = f"B≈ÇƒÖd ≈Çadowania modelu OCR: {str(e)}"
        print(f"‚ùå [OCR_MODELS] {error_msg}")
        logger.error(error_msg)

        # Dodaj stack trace
        import traceback
        traceback.print_exc()

        raise Exception(f"Nie mo≈ºna za≈Çadowaƒá modelu OCR: {str(e)}")


def get_ocr_model() -> Tuple[AutoModelForVision2Seq, AutoProcessor]:
    """Publiczny interfejs do pobierania modelu OCR."""
    print(f"üîç [OCR_MODELS] get_ocr_model wywo≈Çane w procesie PID={os.getpid()}")
    try:
        return _load_once()
    except Exception as e:
        print(f"‚ùå [OCR_MODELS] B≈ÇƒÖd w get_ocr_model: {str(e)}")
        raise


# ---------------------------------------------------------------------------
#  OCR jednej strony
# ---------------------------------------------------------------------------

def _timeout_handler(_signum, _frame):
    raise TimeoutError("Timeout podczas generacji tekstu")


def process_image_to_text(
        image_path: str | Path,
        instruction: str = DEFAULT_OCR_INSTRUCTION,
        model=None,
        processor=None,
):
    """Rozpoznaje tekst z obrazu i zwraca go jako string."""
    print(f"üîç [OCR_MODELS] process_image_to_text wywo≈Çane dla: {image_path}")

    try:
        from qwen_vl_utils import process_vision_info
    except ImportError as e:
        error_msg = f"Nie mo≈ºna zaimportowaƒá qwen_vl_utils: {str(e)}"
        print(f"‚ùå [OCR_MODELS] {error_msg}")
        raise Exception(error_msg)

    # Je≈õli nie podano modelu lub procesora, za≈Çaduj je
    if model is None or processor is None:
        print(f"üîç [OCR_MODELS] ≈Åadowanie modelu i procesora...")
        try:
            model, processor = get_ocr_model()
            print(f"‚úÖ [OCR_MODELS] Model i processor za≈Çadowane")
        except Exception as e:
            error_msg = f"B≈ÇƒÖd ≈Çadowania modelu: {str(e)}"
            print(f"‚ùå [OCR_MODELS] {error_msg}")
            return f"[B≈ÇƒÖd ≈Çadowania modelu: {str(e)}]"

    if isinstance(image_path, Path):
        image_path = str(image_path)

    # Sprawd≈∫ czy plik obrazu istnieje
    if not Path(image_path).exists():
        error_msg = f"Plik obrazu nie istnieje: {image_path}"
        print(f"‚ùå [OCR_MODELS] {error_msg}")
        return f"[B≈ÇƒÖd: {error_msg}]"

    try:
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "You are OCR system for text recognition."}],
            },
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image_path},
                    {"type": "text", "text": instruction},
                ],
            },
        ]

        print(f"üîç [OCR_MODELS] Przetwarzanie wiadomo≈õci...")
        text_prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)

        print(f"üîç [OCR_MODELS] Przygotowywanie inputs...")
        inputs = processor(
            text=[text_prompt],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        ).to(model.device)

        print(f"üîç [OCR_MODELS] Model device: {model.device}, inputs device: {inputs['pixel_values'].device}")
        logger.debug("model=%s pixels=%s", model.device, inputs["pixel_values"].device)

        print(f"üîç [OCR_MODELS] Rozpoczynam generacjƒô tekstu...")
        signal.signal(signal.SIGALRM, _timeout_handler)
        signal.alarm(OCR_TIMEOUT_SECONDS)
        try:
            logger.info("Instrukcja: %s", instruction)
            with torch.no_grad():
                gen_ids = model.generate(
                    **inputs,
                    max_new_tokens=MAX_NEW_TOKENS
                    # eos_token_id=processor.tokenizer.eos_token_id,
                    # pad_token_id=processor.tokenizer.pad_token_id,
                )
            print(f"‚úÖ [OCR_MODELS] Generacja zako≈Ñczona pomy≈õlnie")
        except TimeoutError:
            error_msg = f"Timeout > {OCR_TIMEOUT_SECONDS} s ‚Äì pominiƒôto stronƒô"
            print(f"‚è∞ [OCR_MODELS] {error_msg}")
            logger.error(error_msg)
            return f"[Timeout OCR]"
        finally:
            signal.alarm(0)

        print(f"üîç [OCR_MODELS] Dekodowanie wynik√≥w...")
        trimmed = [o[len(i):] for i, o in zip(inputs.input_ids, gen_ids)]
        text = processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]

        print(f"‚úÖ [OCR_MODELS] OCR zako≈Ñczony, d≈Çugo≈õƒá tekstu: {len(text)}")

        # cleanup RAM
        del inputs, gen_ids, image_inputs, video_inputs
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return text.strip()

    except Exception as e:
        error_msg = f"B≈ÇƒÖd podczas OCR: {str(e)}"
        print(f"‚ùå [OCR_MODELS] {error_msg}")
        logger.error(error_msg)

        # Dodaj stack trace
        import traceback
        traceback.print_exc()

        return f"[B≈ÇƒÖd OCR: {str(e)}]"


# ---------------------------------------------------------------------------
#  Zwalnianie zasob√≥w (legacy helper)
# ---------------------------------------------------------------------------

def clean_resources(*resources: Any) -> None:
    for r in resources:
        if r is not None:
            del r
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()